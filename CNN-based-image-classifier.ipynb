{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # PyTorch deep learning framework\n",
    "import os  # Operating system interface\n",
    "from torch import nn  # Neural network modules\n",
    "import torchvision  # Computer vision library\n",
    "from torchvision import transforms as T  # Image transformations\n",
    "import torchvision.models as models  # Pre-trained models\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize, Resize, CenterCrop, ColorJitter  # Image transformation utilities\n",
    "from torchvision.transforms import v2  # Latest version of transforms\n",
    "from torch.utils.data import DataLoader, Dataset  # Data loading utilities\n",
    "from torch.utils.data.dataset import random_split  # Dataset splitting utility\n",
    "from PIL import Image  # Image processing library\n",
    "import pathlib  # File path handling\n",
    "import numpy as np  # Numerical computing\n",
    "import pandas as pd  # Data manipulation library\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "from tqdm.notebook import tqdm  # Progress bar for notebooks\n",
    "import torchmetrics  # Model evaluation metrics\n",
    "from torchmetrics import Accuracy  # Accuracy metric\n",
    "from torchmetrics.classification import MulticlassF1Score  # F1 score for multiclass\n",
    "from torch.optim import Adam, AdamW  # Optimization algorithms\n",
    "from torchinfo import summary  # Model summary utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = \"data/butterfly_data\"  # Root directory for butterfly dataset\n",
    "N_EPOCHS = 20  # Number of training epochs\n",
    "BATCH_SIZE = 128  # Size of mini-batches for training\n",
    "NUM_CLASSES = 75  # Number of butterfly classes\n",
    "z_dim = 400  # Dimension of latent space\n",
    "LATENT_DIM = 128  # Dimension of latent vectors\n",
    "N_Latent = 10  # Number of latent samples\n",
    "Img_channels = 3  # Number of image channels (RGB)\n",
    "Input_Shape = [3, 128, 128]  # Input image dimensions [channels, height, width]\n",
    "Hidden_dims = 64  # Number of hidden dimensions\n",
    "lr = 1e-3  # Learning rate for optimizer\n",
    "betas = (0.5, 0.999)  # Beta parameters for Adam optimizer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Set device to GPU if available, else CPU\n",
    "print(device)  # Display which device is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/butterfly_data/Training_set.csv\")  # Read training data from CSV file\n",
    "label = df['label']  # Extract labels from dataframe\n",
    "class_to_label = {}  # Initialize dictionary to map class names to numeric labels\n",
    "i = 0  # Initialize counter for label encoding\n",
    "for l in label:  # Iterate through each label\n",
    "    if class_to_label.get((l), -1) == -1:  # Check if label not already mapped\n",
    "        class_to_label[l] = i  # Assign numeric label to class name\n",
    "        i += 1  # Increment counter\n",
    "\n",
    "transform_train = v2.Compose([  # Define training data transformations pipeline\n",
    "    v2.Resize((128, 128)),  # Resize images to 128x128\n",
    "    v2.RandomAffine(degrees=(-30, +30)),  # Apply random affine transformation\n",
    "    v2.RandomRotation(degrees=(-30, +30)),  # Apply random rotation\n",
    "    v2.RandomHorizontalFlip(p=.4),  # Randomly flip images horizontally with 0.4 probability\n",
    "    v2.ToImage(),  # Convert to torch image format\n",
    "    v2.ToDtype(torch.float32, scale=True),  # Convert to float32 and scale pixels\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize using ImageNet statistics\n",
    "                 std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "transform_val = v2.Compose([  # Define validation data transformations pipeline\n",
    "    v2.Resize((128, 128)),  # Resize images to 128x128\n",
    "    v2.ToImage(),  # Convert to torch image format\n",
    "    v2.ToDtype(torch.float32, scale=True),  # Convert to float32 and scale pixels\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize using ImageNet statistics\n",
    "                 std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=class_to_label):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train\n",
    "\n",
    "        if self.train:\n",
    "            self.label_dir = os.path.join(root, \"Training_set.csv\")\n",
    "            self.img_root = os.path.join(root, \"train\")\n",
    "        else:\n",
    "            self.label_dir = os.path.join(root, \"Testing_set.csv\")\n",
    "            self.img_root = os.path.join(root, \"test\")\n",
    "\n",
    "    def __len__(self):\n",
    "        labels = pd.read_csv(self.label_dir)\n",
    "        return len(labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        labels = pd.read_csv(self.label_dir)\n",
    "        img_path = os.path.join(self.img_root, labels.iloc[idx, 0])\n",
    "        img = torchvision.io.read_image(img_path)/255.0\n",
    "        if self.train:\n",
    "            label = labels.iloc[idx, 1]\n",
    "            if self.target_transform:\n",
    "                label = self.target_transform[label]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.train:\n",
    "            return (img, label)\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(\n",
    "    CustomImageDataset(root=dataroot, train=True), [0.8, .2])\n",
    "train_dataset.dataset.transform = transform_train\n",
    "val_dataset.dataset.transform = transform_val\n",
    "print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, Hidden_dims=Hidden_dims):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.Hidden_dims = Hidden_dims\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=self.Hidden_dims,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.BatchNorm2d(self.Hidden_dims),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "            nn.Conv2d(in_channels=self.Hidden_dims, out_channels=2 * \\\n",
    "                      self.Hidden_dims, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.BatchNorm2d(2*self.Hidden_dims),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "            nn.Conv2d(in_channels=2*self.Hidden_dims, out_channels=4 * \\\n",
    "                      self.Hidden_dims, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.BatchNorm2d(4*self.Hidden_dims),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "            nn.Conv2d(in_channels=4*self.Hidden_dims, out_channels=8 * \\\n",
    "                      self.Hidden_dims, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.BatchNorm2d(8*self.Hidden_dims),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "            nn.Conv2d(in_channels=8*self.Hidden_dims, out_channels=16 * \\\n",
    "                      self.Hidden_dims, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.BatchNorm2d(16*self.Hidden_dims),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024*4*4, 512),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 75)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = Classifier().to(device)\n",
    "summary(model_0, (1, 3, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, num_workers  = 2, persistent_workers = True, shuffle = True  )\n",
    "val_dataloader = DataLoader(dataset = val_dataset, batch_size = BATCH_SIZE, num_workers  = 2, persistent_workers = True, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "acc_fn = Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES).to(device)\n",
    "F1_Score = torchmetrics.classification.MulticlassF1Score(\n",
    "    NUM_CLASSES).to(device)\n",
    "opt = AdamW(\n",
    "    model_0.parameters(),\n",
    "    lr=.0001, weight_decay=1e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(dataloader, loss_function, acc_function, opt, model):\n",
    "    model.train()\n",
    "    train_loss, train_acc, train_f1score = 0, 0, 0\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X)\n",
    "        loss = loss_function(y_pred, y)\n",
    "        acc = acc_function(torch.argmax(\n",
    "            torch.softmax(y_pred, dim=1), axis=1), y)\n",
    "        f1score = F1_Score(torch.argmax(\n",
    "            torch.softmax(y_pred, dim=1), axis=1), y)\n",
    "        train_acc += acc\n",
    "        train_f1score += f1score\n",
    "        train_loss += loss\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc /= len(dataloader)\n",
    "    train_f1score /= len(dataloader)\n",
    "    return train_loss, train_acc, train_f1score\n",
    "\n",
    "\n",
    "def val_fn(dataloader, loss_function, acc_function, model):\n",
    "    model.eval()\n",
    "    val_loss, val_acc, val_f1_score = 0, 0, 0\n",
    "    with torch.inference_mode():\n",
    "        for X_test, y_test in dataloader:\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            y_test_pred = model(X_test.to(device))\n",
    "            loss = loss_function(y_test_pred, y_test)\n",
    "            acc = acc_function(torch.argmax(\n",
    "                torch.softmax(y_test_pred, dim=1), axis=1), y_test)\n",
    "            f1_score = F1_Score(torch.argmax(\n",
    "                torch.softmax(y_test_pred, dim=1), axis=1), y_test)\n",
    "            val_acc += acc\n",
    "            val_loss += loss\n",
    "            val_f1_score += f1_score\n",
    "        val_loss /= len(dataloader)\n",
    "        val_acc /= len(dataloader)\n",
    "        val_f1_score /= len(dataloader)\n",
    "    return val_loss, val_acc, val_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_CNN:\n",
    "    NUM_EPOCHS = 20\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        train_loss, train_accuracy, train_f1_score = train_fn(\n",
    "            train_dataloader,\n",
    "            loss_fn,\n",
    "            acc_fn,\n",
    "            opt,\n",
    "            model_0\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_loss, val_accuracy, val_f1_score = val_fn(\n",
    "            val_dataloader,\n",
    "            loss_fn,\n",
    "            acc_fn,\n",
    "            model_0\n",
    "        )\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        print(f\"Epoch {i+1}\")\n",
    "        print(f\"Train: Loss : {train_loss:.4f} Accuracy : {\n",
    "            train_accuracy:.4f} | | Validation: Loss : {val_loss:.4f} Accuracy : {val_accuracy:.4f}\")\n",
    "        print(f\"Train: f1Score : {\n",
    "            train_f1_score:.4f}  | | Validation: f1Score : {val_f1_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ForADRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
